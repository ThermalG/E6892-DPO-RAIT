{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 11407634,
     "sourceType": "datasetVersion",
     "datasetId": 7145844
    }
   ],
   "dockerImageVersionId": 31011,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# EECS E6892 Reinforcement Learning - Final Project\n",
    "### Group DPO-RAIT\n",
    "Qinhao Chen, Alex Wei"
   ]
  },
  {
   "cell_type": "code",
   "source": "#!pip install -U transformers accelerate bitsandbytes",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-24T00:19:29.090571Z",
     "iopub.execute_input": "2025-04-24T00:19:29.090874Z",
     "iopub.status.idle": "2025-04-24T00:21:06.707008Z",
     "shell.execute_reply.started": "2025-04-24T00:19:29.090845Z",
     "shell.execute_reply": "2025-04-24T00:21:06.705984Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\") # we used hugging face to download LLaMA model, please use your own token"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-24T00:24:13.968338Z",
     "iopub.execute_input": "2025-04-24T00:24:13.968625Z",
     "iopub.status.idle": "2025-04-24T00:24:14.718882Z",
     "shell.execute_reply.started": "2025-04-24T00:24:13.968602Z",
     "shell.execute_reply": "2025-04-24T00:24:14.718297Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"--- GPU {i} ---\")\n",
    "    print(f\"Name: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-24T00:24:15.901938Z",
     "iopub.execute_input": "2025-04-24T00:24:15.902579Z",
     "iopub.status.idle": "2025-04-24T00:24:19.163956Z",
     "shell.execute_reply.started": "2025-04-24T00:24:15.902551Z",
     "shell.execute_reply": "2025-04-24T00:24:19.163183Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "\n",
    "save_path = \"./llama3.2-3b\"\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-24T00:24:23.786435Z",
     "iopub.execute_input": "2025-04-24T00:24:23.786886Z",
     "iopub.status.idle": "2025-04-24T00:26:07.270271Z",
     "shell.execute_reply.started": "2025-04-24T00:24:23.786860Z",
     "shell.execute_reply": "2025-04-24T00:26:07.269599Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "\n",
    "# Globals\n",
    "STOP = []\n",
    "SURE = []\n",
    "UNSURE = []\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "results = []\n",
    "\n",
    "def format_subject(subject):\n",
    "    return \" \".join(subject.split(\"_\"))\n",
    "\n",
    "def format_example(input_list):\n",
    "    prompt = input_list[0]\n",
    "    for j in range(len(input_list) - 2):\n",
    "        prompt += f\"\\n{choices[j]}. {input_list[j + 1]}\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def format_shots(prompt_data):\n",
    "    prompt = \"\"\n",
    "    for data in prompt_data:\n",
    "        prompt += data[0]\n",
    "        for j in range(len(data) - 2):\n",
    "            prompt += f\"\\n{choices[j]}. {data[j + 1]}\"\n",
    "        prompt += f\"\\nAnswer:{data[-1]}\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(input_list, subject, prompt_data):\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {format_subject(subject)}.\\n\\n\"\n",
    "    prompt += format_shots(prompt_data)\n",
    "    prompt += format_example(input_list)\n",
    "    return prompt\n",
    "\n",
    "def inference(tokenizer, model, input_text, subject, prompt_data):\n",
    "    full_input = gen_prompt(input_text, subject, prompt_data)\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\")\n",
    "\n",
    "    first_device = next(iter(model.hf_device_map.values())) # move input to model device\n",
    "    inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    generated_token_id = outputs[\"sequences\"][0][-1].item()\n",
    "    generated_token = tokenizer.decode(generated_token_id).strip()\n",
    "    if generated_token in choices:\n",
    "        answer = generated_token\n",
    "        confidence = 1.0\n",
    "    else:\n",
    "        answer = \"UNKNOWN\"\n",
    "        confidence = 0.0\n",
    "\n",
    "    print(f\"[DEBUG] Subject: {subject}\")\n",
    "    print(f\"[DEBUG] Question: {input_text[0]}\")\n",
    "    print(f\"[DEBUG] Predicted Answer: {answer}\")\n",
    "    print(f\"[DEBUG] Reference Answer: {input_text[-1]}\")\n",
    "    print(f\"[DEBUG] Confidence: {confidence}\")\n",
    "\n",
    "    return answer, full_input, confidence\n",
    "\n",
    "def checksure(tokenizer, model, input_text):\n",
    "    full_input = f\"{input_text}. Are you sure you accurately answered the question based on your internal knowledge? I am\"\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=1,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    logits = outputs[\"scores\"][0][0]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=0)\n",
    "    sure_prob = probs[SURE[0]]\n",
    "    unsure_prob = probs[UNSURE[0]]\n",
    "    normalized_conf = sure_prob / (sure_prob + unsure_prob + 1e-6)\n",
    "\n",
    "    generated_id = outputs[\"sequences\"][0][-1].item()\n",
    "    generated_token = tokenizer.decode([generated_id]).strip()\n",
    "    print(f\"[DEBUG] Self-assessment prompt: {full_input}\")\n",
    "    print(f\"[DEBUG] Generated token: '{generated_token}'\")\n",
    "    print(f\"[DEBUG] Token ID: {generated_id}\")\n",
    "    print(f\"[DEBUG] P(sure) = {sure_prob.item():.4f}, P(unsure) = {unsure_prob.item():.4f}\")\n",
    "    print(f\"[DEBUG] Normalized confidence: {normalized_conf.item():.4f}\")\n",
    "    print(f\"SURE[0]: {SURE[0]}, 'sure' tokenized: {tokenizer('sure', add_special_tokens=False)['input_ids']}\")\n",
    "\n",
    "    return normalized_conf.item(), generated_token, generated_id\n",
    "\n",
    "def run_knowledge_boundary_eval(model_path, domain=\"ID\", result_name=\"MMLU\"):\n",
    "    global tokenizer, model, results\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, unk_token=\"<unk>\", bos_token=\"<s>\", eos_token=\"</s>\", add_bos_token=False)\n",
    "    device_map = infer_auto_device_map(model, max_memory={0: \"14GiB\", 1: \"14GiB\"})\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device_map, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "    STOP.clear()\n",
    "    SURE.clear()\n",
    "    UNSURE.clear()\n",
    "    STOP.append(tokenizer(\".\")[\"input_ids\"][0])\n",
    "    SURE.append(tokenizer(\"sure\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "    UNSURE.append(tokenizer(\"unsure\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with open(f\"/kaggle/input/mmlu-test/MMLU_ID_prompt.json\", 'r') as f:\n",
    "        data = json.load(f)\n",
    "    with open(f\"/kaggle/input/mmlu-test/MMLU_ID_train.json\", 'r') as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    for subject in tqdm(data.keys(), desc=\"Subjects\"):\n",
    "        prompt_data = prompt[subject]\n",
    "        for instance in tqdm(data[subject], leave=False, desc=subject):\n",
    "            output, full_input, predict_conf = inference(tokenizer, model, instance, subject, prompt_data)\n",
    "            reference_answer = instance[-1]\n",
    "            correct = 1 if output == reference_answer else 0\n",
    "            results.append({\n",
    "                \"is_correct\": correct,\n",
    "                \"predicted\": output,\n",
    "                \"reference\": reference_answer\n",
    "            })\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    os.makedirs(\"/kaggle/working/results3b\", exist_ok=True)\n",
    "    with open(f\"/kaggle/working/results3b/{result_name}_{domain}_test_inference3b.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-24T00:30:19.353683Z",
     "iopub.execute_input": "2025-04-24T00:30:19.354026Z",
     "iopub.status.idle": "2025-04-24T00:30:19.372981Z",
     "shell.execute_reply.started": "2025-04-24T00:30:19.353994Z",
     "shell.execute_reply": "2025-04-24T00:30:19.372020Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "run_knowledge_boundary_eval(\"./llama3.2-3b\", \"ID\", \"MMLU\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-24T00:30:22.601809Z",
     "iopub.execute_input": "2025-04-24T00:30:22.602140Z",
     "iopub.status.idle": "2025-04-24T00:39:41.182000Z",
     "shell.execute_reply.started": "2025-04-24T00:30:22.602114Z",
     "shell.execute_reply": "2025-04-24T00:39:41.181434Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# results file\n",
    "with open(\"/kaggle/working/results3/MMLU_ID.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# dictionary to hold: {token_id: set of decoded token strings}\n",
    "token_id_to_token = defaultdict(set)\n",
    "\n",
    "for r in results:   # extract all pairs\n",
    "    tid = r.get(\"self_assessed_token_id\")\n",
    "    tok = r.get(\"self_assessed_token\")\n",
    "    if tid is not None and tok is not None:\n",
    "        token_id_to_token[tid].add(tok.strip())\n",
    "\n",
    "# show all unique token IDs and their decoded forms\n",
    "print(\"unique self-assessed tokens and their token IDs:\\n\")\n",
    "for tid, token_set in sorted(token_id_to_token.items()):\n",
    "    print(f\"Token ID {tid}: {', '.join(repr(t) for t in token_set)}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-23T01:44:20.845060Z",
     "iopub.execute_input": "2025-04-23T01:44:20.845277Z",
     "iopub.status.idle": "2025-04-23T01:44:21.075564Z",
     "shell.execute_reply.started": "2025-04-23T01:44:20.845259Z",
     "shell.execute_reply": "2025-04-23T01:44:21.074277Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "with open(\"/kaggle/working/results4/MMLU_ID_inference1.json\", \"r\") as f:\n    data1=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference2.json\", \"r\") as f:\n    data2=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference3.json\", \"r\") as f:\n    data3=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference4.json\", \"r\") as f:\n    data4=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference5.json\", \"r\") as f:\n    data5=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference6.json\", \"r\") as f:\n    data6=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference7.json\", \"r\") as f:\n    data7=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference8.json\", \"r\") as f:\n    data8=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference9.json\", \"r\") as f:\n    data9=json.load(f)\nwith open(\"/kaggle/working/results4/MMLU_ID_inference10.json\", \"r\") as f:\n    data10=json.load(f)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-23T01:44:21.076110Z",
     "iopub.status.idle": "2025-04-23T01:44:21.076385Z",
     "shell.execute_reply.started": "2025-04-23T01:44:21.076242Z",
     "shell.execute_reply": "2025-04-23T01:44:21.076259Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "list = []\n",
    "know = 0\n",
    "rough = 0\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    component = []\n",
    "    count = data1[i][\"is_correct\"] + data2[i][\"is_correct\"] + data3[i][\"is_correct\"] + data4[i][\"is_correct\"] + data5[i][\"is_correct\"] + data6[i][\"is_correct\"] + data7[i][\"is_correct\"] + data8[i][\"is_correct\"] + data9[i][\"is_correct\"] + data10[i][\"is_correct\"]\n",
    "    component.append(count)\n",
    "    if count >=6:\n",
    "        know = know +1\n",
    "        component.append(\"sure\")\n",
    "    else:\n",
    "        component.append(\"unsure\")\n",
    "    list.append(component)\n",
    "    \n",
    "print(know)\n",
    "print(rough)\n",
    "print(len(data1))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-23T01:44:21.077661Z",
     "iopub.status.idle": "2025-04-23T01:44:21.077902Z",
     "shell.execute_reply.started": "2025-04-23T01:44:21.077792Z",
     "shell.execute_reply": "2025-04-23T01:44:21.077802Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "count = 0\n",
    "new_list = []\n",
    "with open(f\"/kaggle/input/mmlu-test/MMLU_ID_train.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "for subject in data.keys():\n",
    "    for instance in data[subject]:\n",
    "        component = instance.copy()\n",
    "        component.append(list[count])\n",
    "        count += 1\n",
    "        new_list.append(component)\n",
    "\n",
    "print(new_list[0])\n",
    "with open(f\"/kaggle/working/results4/MMLU_ID_train10.json\", 'w') as f:\n",
    "    json.dump(new_list, f, indent=2)\n",
    "print(results[0])"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-23T01:44:21.079037Z",
     "iopub.status.idle": "2025-04-23T01:44:21.079344Z",
     "shell.execute_reply.started": "2025-04-23T01:44:21.079187Z",
     "shell.execute_reply": "2025-04-23T01:44:21.079202Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"/kaggle/working/results2/MMLU_ID.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# extract correct predictions\n",
    "num_correct = sum([item[0] for item in results])\n",
    "total = len(results)\n",
    "accuracy = num_correct / total if total > 0 else 0\n",
    "\n",
    "# see a few examples\n",
    "print(f\"Total questions: {total}\")\n",
    "print(f\"Correct predictions: {num_correct}\")\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nSample results:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. Correct: {results[i][0]}, Model Confidence: {results[i][1]:.4f}, Self-assessed: {results[i][2]:.4f}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-23T01:44:21.083859Z",
     "iopub.status.idle": "2025-04-23T01:44:21.084174Z",
     "shell.execute_reply.started": "2025-04-23T01:44:21.084019Z",
     "shell.execute_reply": "2025-04-23T01:44:21.084032Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
